from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base
from sqlalchemy import text
from typing import AsyncGenerator, List
import os
import logging
from .config import settings

logger = logging.getLogger(__name__)

# å»ºç«‹è³‡æ–™åº«å¼•æ“
async_engine = create_async_engine(
    settings.async_database_url,
    echo=settings.debug,
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_size=10,
    max_overflow=20
)

# å»ºç«‹AsyncSessionå·¥å» 
AsyncSessionLocal = async_sessionmaker(
    bind=async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autoflush=True,
    autocommit=False
)

# åŸºç¤æ¨¡å‹é¡
Base = declarative_base()

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    è³‡æ–™åº«ä¾è³´æ³¨å…¥å‡½æ•¸
    ç”¨æ–¼FastAPIè·¯ç”±ä¸­ç²å–è³‡æ–™åº«session
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

async def execute_sql_file(file_path: str):
    """å®‰å…¨åœ°åŸ·è¡ŒSQLæª”æ¡ˆï¼Œåˆ†å‰²å¤šå€‹èªå¥é€ä¸€åŸ·è¡Œ"""
    if not os.path.exists(file_path):
        logger.warning(f"SQLæª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        return
    
    logger.info(f"SQLæª”æ¡ˆåŸ·è¡Œä¸­: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()

        # åˆ†å‰²SQLèªå¥ï¼Œæ­£ç¢ºè™•ç† $$ åŒ…åœçš„å‡½æ•¸
        statements = _split_sql_statements(sql_content)
        
        success_count = 0
        error_count = 0
        
        async with async_engine.begin() as conn:
            for i, statement in enumerate(statements):
                if statement.strip():  # è·³éç©ºèªå¥
                    try:
                        await conn.execute(text(statement))
                        success_count += 1
                        logger.debug(f"åŸ·è¡Œèªå¥ {i+1}/{len(statements)}: {statement[:50]}...")
                    except Exception as stmt_error:
                        error_count += 1
                        error_msg = str(stmt_error).lower()
                        
                        # é€™äº›éŒ¯èª¤æ˜¯å¯å¿½ç•¥çš„ï¼Œå› ç‚ºå®ƒå€‘é€šå¸¸è¡¨ç¤ºè³‡æºå·²å­˜åœ¨
                        ignorable_errors = [
                            'already exists',
                            'duplicate',
                            'current transaction is aborted',
                            'already exists',
                            'relation already exists'
                        ]
                        
                        if any(ignorable in error_msg for ignorable in ignorable_errors):
                            logger.debug(f"å¿½ç•¥é æœŸéŒ¯èª¤ ({i+1}/{len(statements)}): {stmt_error}")
                        else:
                            logger.error(f"åŸ·è¡Œèªå¥å¤±æ•— ({i+1}/{len(statements)}): {stmt_error}")
                            logger.debug(f"å¤±æ•—çš„èªå¥: {statement}")
                        
                        # ç¹¼çºŒåŸ·è¡Œå…¶ä»–èªå¥ï¼Œä¸ä¸­æ–·æ•´å€‹éç¨‹
        
        logger.info(f"SQLæª”æ¡ˆåŸ·è¡Œå®Œæˆ: {file_path} (æˆåŠŸ: {success_count}, éŒ¯èª¤: {error_count})")
        
        if error_count == 0:
            logger.info("âœ… æ‰€æœ‰èªå¥åŸ·è¡ŒæˆåŠŸ")
        elif success_count > 0:
            logger.warning(f"âš ï¸ éƒ¨åˆ†èªå¥åŸ·è¡Œå¤±æ•—ï¼Œä½†æœ‰ {success_count} å€‹èªå¥æˆåŠŸåŸ·è¡Œ")
        else:
            logger.error("âŒ æ‰€æœ‰èªå¥åŸ·è¡Œå¤±æ•—")

    except Exception as e:
        logger.error(f"åŸ·è¡ŒSQLæª”æ¡ˆæ™‚å‡ºéŒ¯ ({file_path}): {e}", exc_info=True)
        raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ï¼Œä½¿æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å¤±æ•—

def _split_sql_statements(sql_content: str) -> List[str]:
    """åˆ†å‰²SQLå…§å®¹ç‚ºç¨ç«‹çš„èªå¥ï¼Œæ­£ç¢ºè™•ç† $$ åŒ…åœçš„å‡½æ•¸"""
    statements = []
    current_statement = ""
    in_dollar_quote = False
    dollar_tag = ""
    
    lines = sql_content.split('\n')
    
    for line in lines:
        stripped_line = line.strip()
        
        # è·³éè¨»è§£å’Œç©ºè¡Œ
        if not stripped_line or stripped_line.startswith('--'):
            current_statement += line + '\n'
            continue
        
        # æª¢æŸ¥ dollar quote çš„é–‹å§‹æˆ–çµæŸ
        if '$$' in line:
            if not in_dollar_quote:
                # é–‹å§‹ dollar quote
                dollar_start = line.find('$$')
                if dollar_start >= 0:
                    # æå–å¯èƒ½çš„æ¨™ç±¤ (ä¾‹å¦‚ $function$ æˆ– $$)
                    before_dollar = line[:dollar_start]
                    after_first_dollar = line[dollar_start+2:]
                    next_dollar = after_first_dollar.find('$$')
                    
                    if next_dollar >= 0:
                        # åŒä¸€è¡Œæœ‰é–‹å§‹å’ŒçµæŸæ¨™ç±¤
                        dollar_tag = after_first_dollar[:next_dollar]
                    else:
                        # åªæœ‰é–‹å§‹æ¨™ç±¤
                        dollar_tag = ""
                        in_dollar_quote = True
            else:
                # æª¢æŸ¥æ˜¯å¦ç‚ºçµæŸæ¨™ç±¤
                expected_end = f"{dollar_tag}$$"
                if expected_end in line:
                    in_dollar_quote = False
                    dollar_tag = ""
        
        current_statement += line + '\n'
        
        # å¦‚æœä¸åœ¨ dollar quote ä¸­ï¼Œæª¢æŸ¥èªå¥çµæŸ
        if not in_dollar_quote and line.strip().endswith(';'):
            statements.append(current_statement.strip())
            current_statement = ""
    
    # åŠ å…¥æœ€å¾Œä¸€å€‹èªå¥ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    if current_statement.strip():
        statements.append(current_statement.strip())
    
    return statements

async def check_table_structure():
    """æª¢æŸ¥é—œéµè¡¨æ ¼çµæ§‹"""
    try:
        async with async_engine.begin() as conn:
            # æª¢æŸ¥sentencesè¡¨æ˜¯å¦æœ‰æ–°æ¬„ä½
            result = await conn.execute(text("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'sentences' 
                AND table_schema = 'public'
                AND column_name IN ('detection_status', 'error_message', 'retry_count', 'explanation')
                ORDER BY column_name;
            """))
            
            existing_columns = [row[0] for row in result.fetchall()]
            required_columns = ['detection_status', 'error_message', 'retry_count', 'explanation']
            missing_columns = [col for col in required_columns if col not in existing_columns]
            
            if missing_columns:
                logger.warning(f"sentencesè¡¨ç¼ºå°‘æ¬„ä½: {missing_columns}")
                return False
            else:
                logger.info("âœ… sentencesè¡¨çµæ§‹æª¢æŸ¥é€šé")
                return True
                
    except Exception as e:
        logger.error(f"æª¢æŸ¥è¡¨æ ¼çµæ§‹å¤±æ•—: {e}")
        return False

async def init_database():
    """åˆå§‹åŒ–è³‡æ–™åº«ï¼ˆç”¨æ–¼æ‡‰ç”¨å•Ÿå‹•æ™‚ï¼‰"""
    try:
        logger.info("ğŸš€ é–‹å§‹åˆå§‹åŒ–è³‡æ–™åº«...")
        
        # 1. æ¸¬è©¦é€£ç·š
        async with async_engine.begin() as conn:
            result = await conn.execute(text("SELECT 1"))
            logger.info("âœ… è³‡æ–™åº«é€£ç·šæ¸¬è©¦æˆåŠŸ")
        
        # 2. ç¢ºä¿UUIDæ“´å±•å­˜åœ¨
        async with async_engine.begin() as conn:
            await conn.execute(text('CREATE EXTENSION IF NOT EXISTS "uuid-ossp";'))
            logger.info("âœ… UUIDæ“´å±•å·²å•Ÿç”¨")
        
        # 3. æª¢æŸ¥æ˜¯å¦å·²æœ‰è¡¨æ ¼å­˜åœ¨
        async with async_engine.begin() as conn:
            result = await conn.execute(text("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'papers'
                );
            """))
            papers_table_exists = result.scalar()
            logger.info(f"ğŸ“Š papers è¡¨æ ¼å­˜åœ¨ç‹€æ…‹: {papers_table_exists}")
        
        # 4. å¦‚æœæ²’æœ‰è¡¨æ ¼ï¼Œç›´æ¥ä½¿ç”¨ schema.sql åˆå§‹åŒ–
        if not papers_table_exists:
            logger.info("ğŸ“‹ æª¢æ¸¬åˆ°ç©ºè³‡æ–™åº«ï¼Œç›´æ¥ä½¿ç”¨ schema.sql åˆå§‹åŒ–...")
            await _fallback_to_schema_sql()
        else:
            # 5. å¦‚æœæœ‰è¡¨æ ¼ï¼Œå˜—è©¦ä½¿ç”¨ Alembic Migration
            logger.info("ğŸ“‹ æª¢æ¸¬åˆ°ç¾æœ‰è³‡æ–™åº«ï¼Œå˜—è©¦åŸ·è¡Œ Alembic é·ç§»...")
            
            try:
                # ä½¿ç”¨Alembicé…ç½®åŸ·è¡Œé·ç§»
                from alembic.config import Config
                from alembic import command
                import os
                
                # å–å¾—migrationç›®éŒ„è·¯å¾‘
                backend_dir = os.path.dirname(os.path.dirname(__file__))
                migrations_dir = os.path.join(backend_dir, "migrations")
                alembic_ini_path = os.path.join(backend_dir, "alembic.ini")
                
                logger.info(f"ğŸ” æª¢æŸ¥Alembicè¨­å®šæª”æ¡ˆ...")
                logger.info(f"  - backend_dir: {backend_dir}")
                logger.info(f"  - migrations_dir: {migrations_dir}")
                logger.info(f"  - alembic_ini_path: {alembic_ini_path}")
                
                if not os.path.exists(alembic_ini_path):
                    logger.warning(f"æ‰¾ä¸åˆ°alembic.iniæª”æ¡ˆ: {alembic_ini_path}ï¼Œè·³é Alembic é·ç§»")
                    raise FileNotFoundError("alembic.ini not found")
                
                if not os.path.exists(migrations_dir):
                    logger.warning(f"æ‰¾ä¸åˆ°migrationsç›®éŒ„: {migrations_dir}ï¼Œè·³é Alembic é·ç§»")
                    raise FileNotFoundError("migrations directory not found")
                
                # å»ºç«‹Alembicé…ç½®
                logger.info("ğŸ”§ å»ºç«‹Alembicé…ç½®...")
                alembic_cfg = Config(alembic_ini_path)
                
                # å¾ç’°å¢ƒè®Šæ•¸å–å¾—è³‡æ–™åº«URL
                host = os.getenv("POSTGRES_HOST", "localhost")
                port = os.getenv("POSTGRES_PORT", "5432")
                database = os.getenv("POSTGRES_DB", "paper_analysis")
                username = os.getenv("POSTGRES_USER", "postgres")
                password = os.getenv("POSTGRES_PASSWORD", "password")
                database_url = f"postgresql://{username}:{password}@{host}:{port}/{database}"
                
                logger.info(f"ğŸ“ è¨­å®šè³‡æ–™åº«URL: {database_url}")
                alembic_cfg.set_main_option("sqlalchemy.url", database_url)
                
                # è¨­å®šæ­£ç¢ºçš„script_locationè·¯å¾‘
                logger.info(f"ğŸ“ è¨­å®šmigrationsè·¯å¾‘: {migrations_dir}")
                alembic_cfg.set_main_option("script_location", migrations_dir)
                
                # æª¢æŸ¥migrationç‹€æ…‹
                logger.info("ğŸ” æª¢æŸ¥migrationç‹€æ…‹...")
                from alembic.migration import MigrationContext
                from sqlalchemy import create_engine
                
                # å»ºç«‹åŒæ­¥å¼•æ“ç”¨æ–¼Alembic
                sync_engine = create_engine(database_url)
                
                with sync_engine.connect() as connection:
                    logger.info("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸï¼Œå»ºç«‹migration context...")
                    context = MigrationContext.configure(connection)
                    
                    # æª¢æŸ¥æ˜¯å¦å·²æœ‰migrationè¨˜éŒ„
                    current_rev = context.get_current_revision()
                    logger.info(f"ğŸ“Š ç›®å‰migrationç‰ˆæœ¬: {current_rev}")
                    
                    if current_rev is None:
                        logger.info("ğŸš€ åˆå§‹åŒ–Alembicç‰ˆæœ¬æ§åˆ¶...")
                        # æ¨™è¨˜ç‚ºå·²åŸ·è¡Œæœ€æ–°migration
                        command.stamp(alembic_cfg, "head")
                        logger.info("âœ… Alembicç‰ˆæœ¬æ§åˆ¶åˆå§‹åŒ–å®Œæˆ")
                    else:
                        logger.info(f"ğŸ“‹ ç™¼ç¾ç¾æœ‰migrationç‰ˆæœ¬: {current_rev}")
                        
                    # åŸ·è¡Œmigrationåˆ°æœ€æ–°ç‰ˆæœ¬
                    logger.info("â¬†ï¸ åŸ·è¡Œmigrationåˆ°æœ€æ–°ç‰ˆæœ¬...")
                    command.upgrade(alembic_cfg, "head")
                    logger.info("âœ… MigrationåŸ·è¡Œå®Œæˆ")
                
                # åŒæ­¥å¼•æ“ç”¨å®Œå³é—œé–‰
                sync_engine.dispose()
                        
            except Exception as migration_error:
                logger.error(f"âŒ Alembicé·ç§»å¤±æ•—: {migration_error}")
                logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(migration_error).__name__}")
                logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {str(migration_error)}")
                import traceback
                logger.error(f"âŒ å®Œæ•´å †ç–Šè¿½è¹¤:")
                logger.error(traceback.format_exc())
                logger.info("ğŸ”„ å›é€€åˆ°schema.sqlæ–¹å¼...")
                
                # å¦‚æœmigrationå¤±æ•—ï¼Œå›é€€åˆ°åŸä¾†çš„schema.sqlæ–¹å¼
                await _fallback_to_schema_sql()
        
        # 6. é©—è­‰æ ¸å¿ƒè¡¨æ ¼æ˜¯å¦å­˜åœ¨
        async with async_engine.begin() as conn:
            tables_to_check = ['papers', 'paper_sections', 'sentences', 'paper_selections', 'processing_queue']
            all_tables_exist = True
            
            for table in tables_to_check:
                result = await conn.execute(text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = '{table}'
                    );
                """))
                exists = result.scalar()
                if exists:
                    logger.info(f"âœ… è¡¨æ ¼ {table} å­˜åœ¨")
                else:
                    logger.error(f"âŒ è¡¨æ ¼ {table} ä¸å­˜åœ¨")
                    all_tables_exist = False
        
        # 7. æª¢æŸ¥è¡¨æ ¼çµæ§‹
        structure_ok = await check_table_structure()
        
        if all_tables_exist and structure_ok:
            logger.info("ğŸ‰ è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼æ‰€æœ‰è¡¨æ ¼å’Œçµæ§‹éƒ½æ­£ç¢º")
        elif all_tables_exist:
            logger.warning("âš ï¸ è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼Œè¡¨æ ¼å­˜åœ¨ä½†çµæ§‹å¯èƒ½æœ‰å•é¡Œ")
        else:
            logger.error("âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—ï¼Œéƒ¨åˆ†è¡¨æ ¼ä¸å­˜åœ¨")
            # æœ€å¾Œå˜—è©¦ï¼šå¼·åˆ¶åŸ·è¡Œ schema
            logger.info("ğŸ”„ æœ€å¾Œå˜—è©¦ï¼šå¼·åˆ¶åŸ·è¡Œå®Œæ•´ schema...")
            await _force_create_schema()
        
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        # ä¸è¦æ‹‹å‡ºç•°å¸¸ï¼Œè®“æ‡‰ç”¨ç¨‹å¼ç¹¼çºŒå•Ÿå‹•
        logger.warning("âš ï¸ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—ï¼Œä½†æ‡‰ç”¨ç¨‹å¼å°‡ç¹¼çºŒå•Ÿå‹•")

async def _fallback_to_schema_sql():
    """å›é€€åˆ°schema.sqlæ–¹å¼å»ºç«‹è¡¨æ ¼"""
    logger.info("ğŸ”„ é–‹å§‹ä½¿ç”¨ schema.sql æ–¹å¼åˆå§‹åŒ–è³‡æ–™åº«...")
    
    try:
        # 1. é¦–å…ˆåŸ·è¡Œä¿®å¾©è…³æœ¬ï¼Œç¢ºä¿ç¼ºå¤±çš„æ¬„ä½è¢«æ·»åŠ 
        fix_script_path = os.path.join(os.path.dirname(__file__), "../database/fix_missing_columns.sql")
        if os.path.exists(fix_script_path):
            logger.info("ğŸ”§ åŸ·è¡Œæ¬„ä½ä¿®å¾©è…³æœ¬...")
            await execute_sql_file(fix_script_path)
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°ä¿®å¾©è…³æœ¬: {fix_script_path}")
        
        # 2. åŸ·è¡Œä¸»è¦schema
        schema_path = os.path.join(os.path.dirname(__file__), "../database/schema.sql")
        if os.path.exists(schema_path):
            logger.info("ğŸ“‹ åŸ·è¡Œä¸»è¦è³‡æ–™åº«schema...")
            await execute_sql_file(schema_path)
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°schemaæª”æ¡ˆ: {schema_path}")
        
        # 3. åŸ·è¡Œå‡ç´šè…³æœ¬
        upgrade_path = os.path.join(os.path.dirname(__file__), "../database/upgrade_sentences_table.sql")
        if os.path.exists(upgrade_path):
            logger.info("â¬†ï¸ åŸ·è¡Œè³‡æ–™åº«å‡ç´šè…³æœ¬...")
            await execute_sql_file(upgrade_path)
        else:
            logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°å‡ç´šè…³æœ¬: {upgrade_path}")
            
        logger.info("âœ… Schema.sql æ–¹å¼åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ Schema.sql åˆå§‹åŒ–å¤±æ•—: {e}")
        raise

async def _force_create_schema():
    """å¼·åˆ¶å‰µå»ºå®Œæ•´çš„è³‡æ–™åº« schema"""
    logger.info("ğŸ”§ å¼·åˆ¶å‰µå»ºå®Œæ•´çš„è³‡æ–™åº« schema...")
    
    schema_sql = """
-- è«–æ–‡åˆ†æç³»çµ±è³‡æ–™åº« Schema
-- PostgreSQL 14+

-- å»ºç«‹UUIDæ“´å±•
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- è«–æ–‡ç®¡ç†è¡¨ (åŠ å…¥TEI XMLå„²å­˜ï¼Œç°¡åŒ–ä½¿ç”¨è€…ç®¡ç†)
CREATE TABLE IF NOT EXISTS papers (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    file_name VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255),
    upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_status VARCHAR(50) DEFAULT 'uploading',
    file_size BIGINT,
    file_hash VARCHAR(64) UNIQUE, -- ç”¨æ–¼æª”æ¡ˆå»é‡
    grobid_processed BOOLEAN DEFAULT FALSE,
    sentences_processed BOOLEAN DEFAULT FALSE,
    od_cd_processed BOOLEAN DEFAULT FALSE, -- æ–°å¢ç¼ºå¤±çš„æ¬„ä½
    pdf_deleted BOOLEAN DEFAULT FALSE, -- æ¨™è¨˜PDFæ˜¯å¦å·²åˆªé™¤
    error_message TEXT,
    -- TEI XML å„²å­˜ (æ–°å¢)
    tei_xml TEXT, -- å„²å­˜å®Œæ•´çš„Grobid TEI XML
    tei_metadata JSONB, -- å„²å­˜è§£æå¾Œçš„metadata (ä½œè€…ã€æ¨™é¡Œç­‰)
    processing_completed_at TIMESTAMP, -- è™•ç†å®Œæˆæ™‚é–“
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è«–æ–‡ç« ç¯€è¡¨ (å¾TEI XMLè§£æ)
CREATE TABLE IF NOT EXISTS paper_sections (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    paper_id UUID NOT NULL REFERENCES papers(id) ON DELETE CASCADE,
    section_type VARCHAR(50) NOT NULL, -- abstract, introduction, methodology, results, conclusion, references
    page_num INTEGER,
    content TEXT NOT NULL,
    section_order INTEGER, -- ç« ç¯€é †åº
    tei_coordinates JSONB, -- TEIä¸­çš„åº§æ¨™è³‡è¨Š
    word_count INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å¥å­è¡¨ (å¾ç« ç¯€å…§å®¹åˆ‡åˆ†)
CREATE TABLE IF NOT EXISTS sentences (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    paper_id UUID NOT NULL REFERENCES papers(id) ON DELETE CASCADE,
    section_id UUID REFERENCES paper_sections(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    sentence_order INTEGER, -- åœ¨è©²ç« ç¯€ä¸­çš„é †åº
    word_count INTEGER,
    char_count INTEGER,
    -- æª¢æ¸¬çµæœæ¬„ä½
    has_objective BOOLEAN DEFAULT NULL,
    has_dataset BOOLEAN DEFAULT NULL,
    has_contribution BOOLEAN DEFAULT NULL,
    detection_status VARCHAR(20) DEFAULT 'unknown', -- pending, completed, failed, unknown
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    explanation TEXT, -- æª¢æ¸¬çµæœçš„è§£é‡‹
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è«–æ–‡é¸å–ç‹€æ…‹è¡¨ (ç”¨æ–¼æ¨™è¨˜å“ªäº›è«–æ–‡è¢«é¸ä¸­é€²è¡Œåˆ†æ)
CREATE TABLE IF NOT EXISTS paper_selections (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    paper_id UUID NOT NULL REFERENCES papers(id) ON DELETE CASCADE,
    is_selected BOOLEAN DEFAULT FALSE,
    selected_at TIMESTAMP,
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(paper_id) -- æ¯ç¯‡è«–æ–‡åªèƒ½æœ‰ä¸€å€‹é¸å–ç‹€æ…‹
);

-- è™•ç†ä½‡åˆ—è¡¨ (ç”¨æ–¼ç®¡ç†æª”æ¡ˆè™•ç†ä»»å‹™)
CREATE TABLE IF NOT EXISTS processing_queue (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    paper_id UUID NOT NULL REFERENCES papers(id) ON DELETE CASCADE,
    task_type VARCHAR(50) NOT NULL, -- grobid_processing, sentence_extraction, od_cd_detection
    status VARCHAR(20) DEFAULT 'pending', -- pending, processing, completed, failed
    priority INTEGER DEFAULT 0,
    retry_count INTEGER DEFAULT 0,
    max_retries INTEGER DEFAULT 3,
    error_message TEXT,
    task_data JSONB, -- ä»»å‹™ç›¸é—œçš„é¡å¤–è³‡æ–™
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP
);

-- å»ºç«‹ç´¢å¼•ä»¥æå‡æŸ¥è©¢æ•ˆèƒ½
CREATE INDEX IF NOT EXISTS idx_papers_status ON papers(processing_status);
CREATE INDEX IF NOT EXISTS idx_papers_hash ON papers(file_hash);
CREATE INDEX IF NOT EXISTS idx_papers_created ON papers(created_at);
CREATE INDEX IF NOT EXISTS idx_sections_paper ON paper_sections(paper_id);
CREATE INDEX IF NOT EXISTS idx_sections_type ON paper_sections(section_type);
CREATE INDEX IF NOT EXISTS idx_sentences_paper ON sentences(paper_id);
CREATE INDEX IF NOT EXISTS idx_sentences_section ON sentences(section_id);
CREATE INDEX IF NOT EXISTS idx_sentences_detection ON sentences(has_objective, has_dataset, has_contribution);
CREATE INDEX IF NOT EXISTS idx_sentences_status ON sentences(detection_status);
CREATE INDEX IF NOT EXISTS idx_selections_paper ON paper_selections(paper_id);
CREATE INDEX IF NOT EXISTS idx_selections_selected ON paper_selections(is_selected);
CREATE INDEX IF NOT EXISTS idx_queue_status ON processing_queue(status);
CREATE INDEX IF NOT EXISTS idx_queue_type ON processing_queue(task_type);
CREATE INDEX IF NOT EXISTS idx_queue_priority ON processing_queue(priority DESC, created_at);
"""
    
    try:
        async with async_engine.begin() as conn:
            await conn.execute(text(schema_sql))
            logger.info("âœ… å¼·åˆ¶ schema å‰µå»ºå®Œæˆ")
            
        # å‰µå»ºè§¸ç™¼å™¨
        trigger_sql = """
-- å»ºç«‹è§¸ç™¼å™¨ä»¥è‡ªå‹•æ›´æ–° updated_at æ¬„ä½
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- ç‚º sentences è¡¨å»ºç«‹è§¸ç™¼å™¨
DROP TRIGGER IF EXISTS update_sentences_updated_at ON sentences;
CREATE TRIGGER update_sentences_updated_at
    BEFORE UPDATE ON sentences
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
"""
        
        async with async_engine.begin() as conn:
            await conn.execute(text(trigger_sql))
            logger.info("âœ… è§¸ç™¼å™¨å‰µå»ºå®Œæˆ")
            
    except Exception as e:
        logger.error(f"âŒ å¼·åˆ¶ schema å‰µå»ºå¤±æ•—: {e}")
        raise

async def close_database():
    """é—œé–‰è³‡æ–™åº«é€£ç·š"""
    try:
        await async_engine.dispose()
        logger.info("ğŸ‘‹ è³‡æ–™åº«é€£ç·šå·²é—œé–‰")
    except Exception as e:
        logger.error(f"é—œé–‰è³‡æ–™åº«é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}") 