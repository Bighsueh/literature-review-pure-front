"""
æª”æ¡ˆè™•ç†æµæ°´ç·šæœå‹™
æ•´åˆGrobid,N8N APIå’Œè³‡æ–™åº«æœå‹™çš„å®Œæ•´è™•ç†æµç¨‹
"""

import asyncio
import time
import json
import uuid
from typing import Dict, List, Any, Optional, Tuple
from sqlalchemy.ext.asyncio import AsyncSession
from datetime import datetime
import os
from sqlalchemy import select, and_, or_, update
from sqlalchemy.sql import func

from ..core.config import settings
from ..core.logging import get_logger
from ..services.grobid_service import grobid_service
from ..services.n8n_service import n8n_service
from ..services.split_sentences_service import split_sentences_service
from ..services.queue_service import queue_service, QueueTask, TaskPriority
from ..services.db_service import db_service
from ..core.database import get_db
from ..models.paper import Paper, PaperSection, Sentence
from sqlalchemy.dialects.postgresql import insert as pg_insert
from pathlib import Path

logger = get_logger("processing_service")

class ProcessingSteps:
    """è™•ç†æ­¥é©Ÿå®šç¾©"""
    FILE_VALIDATION = "file_validation"
    GROBID_PROCESSING = "grobid_processing"
    SECTION_ANALYSIS = "section_analysis"
    SENTENCE_EXTRACTION = "sentence_extraction"
    OD_CD_DETECTION = "od_cd_detection"
    KEYWORD_EXTRACTION = "keyword_extraction"
    DATABASE_STORAGE = "database_storage"
    CLEANUP = "cleanup"

class ProcessingService:
    """æª”æ¡ˆè™•ç†æµæ°´ç·šæœå‹™"""
    
    def __init__(self):
        self.total_processing_steps = 8
        
        # è¨»å†Šä»»å‹™è™•ç†å™¨
        self._register_handlers()
        
        logger.info("æª”æ¡ˆè™•ç†æœå‹™åˆå§‹åŒ–å®Œæˆ")
    
    def _register_handlers(self):
        """è¨»å†Šä»»å‹™è™•ç†å™¨åˆ°ä½‡åˆ—æœå‹™"""
        queue_service.register_handler("file_processing", self._process_file)
        queue_service.register_handler("batch_sentence_analysis", self._batch_sentence_analysis)
        queue_service.register_handler("section_reprocessing", self._reprocess_sections)
        
        logger.info("ä»»å‹™è™•ç†å™¨å·²è¨»å†Š")
    
    # ===== ä¸»è¦è™•ç†æµç¨‹ =====
    
    async def process_file(
        self,
        file_id: str,
        user_id: str = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        options: Dict[str, Any] = None,
        session: Optional[AsyncSession] = None  # æ–°å¢å¯é¸çš„æœƒè©±åƒæ•¸
    ) -> str:
        """
        é–‹å§‹æª”æ¡ˆè™•ç†æµç¨‹
        
        Args:
            file_id: æª”æ¡ˆID
            user_id: ä½¿ç”¨è€…ID
            priority: ä»»å‹™å„ªå…ˆåº
            options: è™•ç†é¸é …
            session: å¯é¸çš„è³‡æ–™åº«æœƒè©±ï¼Œå¦‚æœæä¾›å‰‡æœƒåœ¨å‰µå»ºä»»å‹™å‰ç¢ºä¿æäº¤
            
        Returns:
            ä»»å‹™ID
        """
        # å¦‚æœæœ‰æœƒè©±å‚³å…¥ï¼Œç¢ºä¿åœ¨å‰µå»ºä»»å‹™å‰æäº¤æ‰€æœ‰è®Šæ›´
        if session:
            if session.dirty or session.new or session.deleted:
                await session.commit()
                logger.debug("åœ¨å‰µå»ºè™•ç†ä»»å‹™å‰å·²æäº¤æœƒè©±è®Šæ›´")
        
        processing_options = {
            "extract_keywords": True,
            "detect_od_cd": True,
            "analyze_sections": True,
            "max_sentences_per_batch": 50,
            **(options or {})
        }
        
        task_data = {
            "file_id": file_id,
            "options": processing_options
        }
        
        task_id = await queue_service.add_task(
            task_type="file_processing",
            data=task_data,
            priority=priority,
            user_id=user_id,
            file_id=file_id,
            timeout_seconds=1800  # 30åˆ†é˜è¶…æ™‚
        )
        
        logger.info(f"æª”æ¡ˆè™•ç†ä»»å‹™å·²å‰µå»º: {task_id} (æª”æ¡ˆ: {file_id})")
        return task_id
    
    async def _process_file(self, task: QueueTask) -> Dict[str, Any]:
        """
        æª”æ¡ˆè™•ç†ä¸»æµç¨‹ (å¯æ¢å¾©)
        å”èª¿å„å€‹è™•ç†æ­¥é©Ÿï¼Œä¸¦å…è¨±å¾å¤±æ•—é»æ¢å¾©
        """
        file_id = task.data["file_id"]
        options = task.data["options"]
        logger.info(f"é–‹å§‹è™•ç†æª”æ¡ˆ (å¯æ¢å¾©æµç¨‹): {file_id}")

        from ..core.database import AsyncSessionLocal
        session = AsyncSessionLocal()
        try:
            # ç²å–æœ€æ–°çš„è«–æ–‡ç‹€æ…‹ (åŠ é–é¿å…ä½µç™¼)
            paper = await db_service.get_paper_by_id(session, file_id)
            if not paper:
                raise ValueError(f"è™•ç†é–‹å§‹æ™‚æ‰¾ä¸åˆ°è«–æ–‡è¨˜éŒ„: {file_id}")

            # é©—è­‰æª”æ¡ˆä¸¦ç²å–æª”æ¡ˆè³‡è¨Šï¼ˆåœ¨æ‰€æœ‰æ­¥é©Ÿé–‹å§‹å‰ï¼‰
            file_info = await self._validate_file(file_id)
            logger.info(f"æª”æ¡ˆé©—è­‰å®Œæˆ: {file_id} - æª”æ¡ˆè·¯å¾‘: {file_info['file_path']}")

            # ===== æ­¥é©Ÿ 1: Grobid TEI è§£æ =====
            if (not paper.grobid_processed) or (not paper.tei_xml):
                await queue_service.update_progress(task.task_id, step_name="Grobid TEI è§£æ")
                grobid_result = await self._process_with_grobid(file_info)

                # å¢é‡å„²å­˜ Grobid çµæœ
                await db_service.update_paper_grobid_results(
                    session,
                    paper_id=file_id,
                    grobid_result=grobid_result,
                    status="processing"
                )
                await session.commit()
                # é‡æ–°è¼‰å…¥ paper
                paper = await db_service.get_paper_by_id(session, file_id)
                logger.info(f"[é€²åº¦] Grobid è™•ç†å®Œæˆä¸¦å·²å„²å­˜: {file_id}")
            else:
                logger.info(f"å·²å­˜åœ¨ Grobid çµæœï¼Œè·³é TEI è§£æ: {file_id}")

            # ===== æ­¥é©Ÿ 2: ç« ç¯€èˆ‡å¥å­æå– =====
            if not paper.sentences_processed:
                await queue_service.update_progress(task.task_id, step_name="ç« ç¯€èˆ‡å¥å­æå–")

                # é€éé‡è©¦æ©Ÿåˆ¶ç¢ºä¿èƒ½æ‹¿åˆ° TEI XML
                grobid_xml = await self._get_tei_xml_with_retry(session, file_id)

                # ä½¿ç”¨ grobid_service è§£æ XML ä»¥ç²å–ç« ç¯€
                pdf_path = file_info["file_path"]
                sections = await grobid_service.parse_sections_from_xml(grobid_xml, pdf_path)
                if not sections:
                    raise ValueError(f"å¾ TEI XML è§£æç« ç¯€å¤±æ•—: {file_id}")
                
                logger.info(f"å¾ TEI XML è§£æå‡º {len(sections)} å€‹ç« ç¯€ - paper_id: {file_id}")
                grobid_result_mock = {"sections": sections}

                sections_analysis = await self._analyze_sections(grobid_result_mock, options)
                if not sections_analysis:
                    raise ValueError(f"ç« ç¯€åˆ†æå¤±æ•—: {file_id}")
                    
                sentences_data = await self._extract_sentences(sections_analysis)
                if not sentences_data:
                    raise ValueError(f"å¥å­æå–å¤±æ•—: {file_id}")
                
                logger.info(f"æº–å‚™å„²å­˜ {len(sections_analysis)} å€‹ç« ç¯€å’Œ {len(sentences_data)} å€‹å¥å­ - paper_id: {file_id}")
                
                # å¢é‡å„²å­˜ç« ç¯€èˆ‡å¥å­ - é€™è£¡æœƒè‡ªå‹•æäº¤å’Œé©—è­‰
                await db_service.save_sections_and_sentences(
                    session,
                    paper_id=file_id,
                    sections_analysis=sections_analysis,
                    sentences_data=sentences_data,
                )
                
                # é‡æ–°ç²å–è«–æ–‡ç‹€æ…‹ä»¥ç¢ºä¿æ›´æ–°
                await session.commit()
                
                # æ‰‹å‹•ç¢ºä¿ sentences_processed ç‹€æ…‹æ­£ç¢ºè¨­ç½®
                await db_service.update_paper_status(
                    session, 
                    file_id, 
                    "processing", 
                    error_message=None
                )
                
                # é¡å¤–æª¢æŸ¥ï¼šç¢ºä¿ sentences_processed æ¨™è¨˜ç‚º True
                check_result = await session.execute(
                    select(Paper.sentences_processed).where(Paper.id == file_id)
                )
                is_sentences_processed = check_result.scalar()
                
                if not is_sentences_processed:
                    logger.warning(f"sentences_processed ç‹€æ…‹æœªæ­£ç¢ºè¨­ç½®ï¼Œæ‰‹å‹•ä¿®æ­£: {file_id}")
                    await session.execute(
                        update(Paper).where(Paper.id == file_id).values(sentences_processed=True)
                    )
                    await session.commit()
                
                paper = await db_service.get_paper_by_id(session, file_id)
                logger.info(f"[é€²åº¦] ç« ç¯€èˆ‡å¥å­æå–å®Œæˆä¸¦å·²å„²å­˜: {file_id} - sentences_processed: {paper.sentences_processed}")
            else:
                # æ­¥é©Ÿ 2.5: é ç¢¼æ›´æ–°ï¼ˆå¯æ¢å¾©æµç¨‹ä¸­çš„é¡å¤–æ­¥é©Ÿï¼‰
                await queue_service.update_progress(task.task_id, step_name="é ç¢¼è³‡è¨Šæ›´æ–°")
                # ä¿®å¾©è·¯å¾‘å•é¡Œ - é‡ç”¨ä¹‹å‰é©—è­‰éçš„æª”æ¡ˆè·¯å¾‘
                pdf_path = file_info["file_path"]
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°é ç¢¼ï¼ˆå¦‚æœæ‰€æœ‰ç« ç¯€çš„é ç¢¼éƒ½æ˜¯ NULLï¼‰
                existing_sections = await db_service.get_sections_for_paper(session, file_id)
                needs_page_update = any(section.page_num is None for section in existing_sections)
                
                if needs_page_update:
                    logger.info(f"æª¢æ¸¬åˆ°ç« ç¯€ç¼ºå°‘é ç¢¼è³‡è¨Šï¼Œé–‹å§‹ PDF åˆ†ææ›´æ–°: {file_id}")
                    page_update_success = await self._update_section_page_numbers(session, file_id, pdf_path)
                    if page_update_success:
                        logger.info(f"[é€²åº¦] é ç¢¼è³‡è¨Šæ›´æ–°å®Œæˆ: {file_id}")
                    else:
                        logger.warning(f"é ç¢¼è³‡è¨Šæ›´æ–°å¤±æ•—ï¼Œä½†ä¸å½±éŸ¿ä¸»æµç¨‹: {file_id}")
                else:
                    logger.info(f"ç« ç¯€é ç¢¼è³‡è¨Šå·²å­˜åœ¨ï¼Œè·³éæ›´æ–°: {file_id}")
            
            # æ­¥é©Ÿ 3: OD/CD æª¢æ¸¬
            # é‡æ–°ç²å– paper å°è±¡ä»¥é¿å…ç•°æ­¥å•é¡Œ
            paper = await db_service.get_paper_by_id(session, file_id)
            if not paper.od_cd_processed and options.get("detect_od_cd", True):
                await queue_service.update_progress(task.task_id, step_name="OD/CD æª¢æ¸¬")
                
                # é©—è­‰å¥å­è³‡æ–™æ˜¯å¦çœŸçš„å­˜åœ¨
                all_sentences = await db_service.get_sentences_for_paper(session, file_id)
                if not all_sentences:
                    raise ValueError(f"ç„¡æ³•ç²å–å¥å­è³‡æ–™é€²è¡Œ OD/CD æª¢æ¸¬: {file_id}")
                
                logger.info(f"é–‹å§‹ OD/CD æª¢æ¸¬ï¼Œå¥å­æ•¸é‡: {len(all_sentences)} - paper_id: {file_id}")
                
                od_cd_results = await self._detect_od_cd(all_sentences, {}) # grobid_result åœ¨æ­¤ä¸éœ€è¦
                
                # å¢é‡å„²å­˜ OD/CD çµæœ
                await db_service.save_od_cd_results(
                    session,
                    paper_id=file_id,
                    od_cd_results=od_cd_results
                )
                await session.commit()
                logger.info(f"[é€²åº¦] OD/CD æª¢æ¸¬å®Œæˆä¸¦å·²å„²å­˜: {file_id}")
            
            # æ­¥é©Ÿ 4: æœ€çµ‚é©—è­‰èˆ‡è™•ç†
            await queue_service.update_progress(task.task_id, step_name="æœ€çµ‚é©—è­‰")
            
            # é—œéµä¿®å¾©ï¼šåœ¨æ¨™è¨˜ç‚ºå®Œæˆå‰é©—è­‰æ‰€æœ‰è³‡æ–™çœŸçš„å­˜åœ¨
            final_verification = await self._verify_processing_completion(session, file_id)
            if not final_verification["success"]:
                error_msg = f"è™•ç†å®Œæˆé©—è­‰å¤±æ•—: {final_verification['error']}"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            logger.info(f"è™•ç†å®Œæˆé©—è­‰é€šé - {final_verification['summary']}")
            
            # æ­¥é©Ÿ 5: å®Œæˆè™•ç†èˆ‡æ¸…ç†
            await queue_service.update_progress(task.task_id, step_name="å®Œæˆè™•ç†")
            
            # æ›´æ–°æœ€çµ‚ç‹€æ…‹
            await db_service.update_paper_status(session, file_id, "completed")
            
            # æ¸…ç†æš«å­˜æª”æ¡ˆ
            await self._cleanup_temp_files(file_info)

            logger.info(f"æª”æ¡ˆè™•ç†å®Œæˆ: {file_id}")
            return {"status": "completed", "file_id": file_id, "verification": final_verification}

        except Exception as e:
            logger.error(f"æª”æ¡ˆè™•ç†å¤±æ•—: {file_id} - {e}", exc_info=True)
            # åœ¨å–®ç¨çš„æœƒè©±ä¸­æ›´æ–°éŒ¯èª¤ç‹€æ…‹ï¼Œä»¥é˜²ä¸»æœƒè©±å·²å›æ»¾
            error_session = AsyncSessionLocal()
            try:
                await db_service.update_paper_status(error_session, file_id, "error", str(e))
                await error_session.commit()
            except Exception as update_error:
                logger.error(f"æ›´æ–°éŒ¯èª¤ç‹€æ…‹å¤±æ•—: {update_error}")
            finally:
                await error_session.close()
            raise
        finally:
            await session.close()
    
    async def _process_file_legacy(self, task: QueueTask) -> Dict[str, Any]:
        """æª”æ¡ˆè™•ç†ä¸»æµç¨‹ (èˆŠç‰ˆï¼Œå–®é«”å¼)"""
        file_id = task.data["file_id"]
        options = task.data["options"]
        
        logger.info(f"é–‹å§‹è™•ç†æª”æ¡ˆ: {file_id}")
        
        # åˆå§‹åŒ–é€²åº¦
        await queue_service.update_progress(
            task.task_id,
            current_step=0,
            total_steps=self.total_processing_steps,
            step_name="é–‹å§‹è™•ç†",
            details={"file_id": file_id}
        )
        
        try:
            # æ­¥é©Ÿ 1: æª”æ¡ˆé©—è­‰
            await queue_service.update_progress(
                task.task_id,
                current_step=1,
                step_name="æª”æ¡ˆé©—è­‰"
            )
            
            file_info = await self._validate_file(file_id)
            
            # æ­¥é©Ÿ 2: Grobid è™•ç†
            await queue_service.update_progress(
                task.task_id,
                current_step=2,
                step_name="Grobid TEI è§£æ"
            )
            
            grobid_result = await self._process_with_grobid(file_info)
            
            # æ­¥é©Ÿ 3: ç« ç¯€åˆ†æ
            await queue_service.update_progress(
                task.task_id,
                current_step=3,
                step_name="ç« ç¯€åˆ†æ"
            )
            
            sections_analysis = await self._analyze_sections(grobid_result, options)
            
            # æ­¥é©Ÿ 4: å¥å­æå–
            await queue_service.update_progress(
                task.task_id,
                current_step=4,
                step_name="å¥å­æå–"
            )
            
            sentences_data = await self._extract_sentences(sections_analysis)
            
            # æ­¥é©Ÿ 5: OD/CD æª¢æ¸¬ (å¯é¸)
            od_cd_results = None
            if options.get("detect_od_cd", True):
                await queue_service.update_progress(
                    task.task_id,
                    current_step=5,
                    step_name="OD/CD æª¢æ¸¬"
                )
                
                od_cd_results = await self._detect_od_cd(sentences_data, grobid_result)
            
            # æ­¥é©Ÿ 6: é—œéµè©æå– (å¯é¸)
            keyword_results = None
            if options.get("extract_keywords", True):
                await queue_service.update_progress(
                    task.task_id,
                    current_step=6,
                    step_name="é—œéµè©æå–"
                )
                
                keyword_results = await self._extract_keywords(sections_analysis)
            
            # æ­¥é©Ÿ 7: è³‡æ–™åº«å­˜å„²
            await queue_service.update_progress(
                task.task_id,
                current_step=7,
                step_name="è³‡æ–™åº«å­˜å„²"
            )
            
            storage_result = await self._store_results(
                paper_id=file_id,
                grobid_result=grobid_result,
                sections_analysis=sections_analysis,
                sentences_data=sentences_data,
                od_cd_results=od_cd_results,
                keyword_results=keyword_results
            )
            
            # æ­¥é©Ÿ 8: æ¸…ç†
            await queue_service.update_progress(
                task.task_id,
                current_step=8,
                step_name="è™•ç†å®Œæˆ"
            )
            
            await self._cleanup_temp_files(file_info)
            
            # ç·¨è­¯æœ€çµ‚çµæœ
            result = {
                "file_id": file_id,
                "processing_completed_at": datetime.now().isoformat(),
                "grobid_summary": {
                    "title": grobid_result.get("title"),
                    "authors": grobid_result.get("authors", []),
                    "sections_count": len(grobid_result.get("sections", [])),
                    "references_count": len(grobid_result.get("references", []))
                },
                "analysis_summary": {
                    "total_sentences": len(sentences_data),
                    "od_cd_detected": len([s for s in (od_cd_results or []) if s.get("is_od_cd")]) if od_cd_results else None,
                    "keywords_extracted": len(keyword_results or []),
                    "sections_processed": len(sections_analysis)
                },
                "storage_info": storage_result
            }
            
            logger.info(f"æª”æ¡ˆè™•ç†å®Œæˆ: {file_id}")
            return result
            
        except Exception as e:
            logger.error(f"æª”æ¡ˆè™•ç†å¤±æ•—: {file_id} - {e}")
            raise
    
    # ===== å€‹åˆ¥è™•ç†æ­¥é©Ÿ =====
    
    async def _validate_file(self, file_id: str) -> Dict[str, Any]:
        """æª”æ¡ˆé©—è­‰"""
        # ç›´æ¥å¾è³‡æ–™åº«ç²å–æª”æ¡ˆè³‡è¨Š
        from ..core.database import AsyncSessionLocal
        from sqlalchemy.ext.asyncio import AsyncSession
        
        # å‰µå»ºä¸€å€‹æ–°çš„è³‡æ–™åº«æœƒè©±ä¾†ç²å–æª”æ¡ˆè³‡è¨Š
        async with AsyncSessionLocal() as db:
            try:
                paper = await db_service.get_paper_by_id(db, file_id)
                
                if not paper:
                    # è¨˜éŒ„è©³ç´°éŒ¯èª¤ä¿¡æ¯ä¸¦æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡ä»»å‹™
                    logger.error(f"æª”æ¡ˆè¨˜éŒ„ä¸å­˜åœ¨æ–¼è³‡æ–™åº«: {file_id}")
                    logger.error(f"å¯èƒ½åŸå› : 1) è¨˜éŒ„å·²è¢«åˆªé™¤ 2) IDéŒ¯èª¤ 3) è³‡æ–™åº«äº‹å‹™å•é¡Œ")
                    
                    # åœæ­¢ç›¸é—œçš„é‡è¤‡ä»»å‹™
                    await self._cleanup_failed_task(file_id)
                    raise ValueError(f"æª”æ¡ˆè¨˜éŒ„ä¸å­˜åœ¨æ–¼è³‡æ–™åº«: {file_id}")
                
                # æª¢æŸ¥è«–æ–‡ç‹€æ…‹
                if paper.processing_status == "error":
                    logger.warning(f"æª”æ¡ˆç‹€æ…‹ç‚ºéŒ¯èª¤ï¼Œåœæ­¢è™•ç†: {file_id} - {paper.error_message}")
                    await self._cleanup_failed_task(file_id)
                    # é¿å…éŒ¯èª¤è¨Šæ¯ç–ŠåŠ ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŒ¯èª¤è¨Šæ¯
                    original_error = paper.error_message or "æª”æ¡ˆè™•ç†å¤±æ•—"
                    # æå–æœ€åˆçš„éŒ¯èª¤è¨Šæ¯ï¼ˆå»é™¤ç–ŠåŠ çš„å‰ç¶´ï¼‰
                    if "æª”æ¡ˆç‹€æ…‹ç‚ºéŒ¯èª¤:" in original_error:
                        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å¯¦éš›éŒ¯èª¤è¨Šæ¯
                        parts = original_error.split("æª”æ¡ˆç‹€æ…‹ç‚ºéŒ¯èª¤:")
                        original_error = parts[-1].strip()
                    raise ValueError(original_error)
                
                # ä¿®å¾©ï¼šæ”¯æ´å·¥ä½œå€åŒ–çš„æª”æ¡ˆè·¯å¾‘
                # å…ˆå˜—è©¦å·¥ä½œå€åŒ–è·¯å¾‘ï¼Œå†å˜—è©¦å‚³çµ±è·¯å¾‘
                possible_paths = []
                
                # 1. å·¥ä½œå€åŒ–è·¯å¾‘ï¼ˆæ–°ç‰ˆï¼‰
                if paper.workspace_id:
                    workspace_path = os.path.join(settings.temp_files_dir, str(paper.workspace_id), paper.file_name)
                    possible_paths.append(workspace_path)
                
                # 2. å‚³çµ±è·¯å¾‘ï¼ˆèˆŠç‰ˆç›¸å®¹ï¼‰
                traditional_path = os.path.join(settings.temp_files_dir, paper.file_name)
                possible_paths.append(traditional_path)
                
                # 3. ç”¨æª”æ¡ˆé›œæ¹Šå€¼æœå°‹ç›¸ç¬¦çš„æª”æ¡ˆ
                if paper.file_hash:
                    import glob
                    # æœå°‹æ‰€æœ‰å¯èƒ½çš„æª”æ¡ˆè·¯å¾‘
                    hash_pattern = os.path.join(settings.temp_files_dir, "**", f"{paper.file_hash}_*.pdf")
                    matching_files = glob.glob(hash_pattern, recursive=True)
                    possible_paths.extend(matching_files)
                
                # æ‰¾åˆ°ç¬¬ä¸€å€‹å­˜åœ¨çš„æª”æ¡ˆ
                file_path = None
                for path in possible_paths:
                    if os.path.exists(path):
                        file_path = path
                        logger.info(f"æ‰¾åˆ°æª”æ¡ˆ: {file_path}")
                        break
                
                if not file_path:
                    logger.error(f"æª”æ¡ˆå¯¦é«”ä¸å­˜åœ¨ï¼Œå·²å˜—è©¦ä»¥ä¸‹è·¯å¾‘:")
                    for i, path in enumerate(possible_paths, 1):
                        logger.error(f"  {i}. {path}")
                    
                    # åˆ—å‡º temp_files ç›®éŒ„å…§å®¹ç”¨æ–¼èª¿è©¦
                    try:
                        files = os.listdir(settings.temp_files_dir)
                        logger.error(f"temp_files ç›®éŒ„å…§å®¹ (å‰10å€‹): {files[:10]}")
                        
                        # å¦‚æœæœ‰å·¥ä½œå€ï¼Œä¹Ÿåˆ—å‡ºå·¥ä½œå€ç›®éŒ„å…§å®¹
                        if paper.workspace_id:
                            workspace_dir = os.path.join(settings.temp_files_dir, str(paper.workspace_id))
                            if os.path.exists(workspace_dir):
                                workspace_files = os.listdir(workspace_dir)
                                logger.error(f"å·¥ä½œå€ç›®éŒ„å…§å®¹: {workspace_files[:10]}")
                    except Exception as e:
                        logger.error(f"ç„¡æ³•åˆ—å‡ºç›®éŒ„å…§å®¹: {e}")
                    
                    # æ›´æ–°è³‡æ–™åº«ç‹€æ…‹ç‚ºéŒ¯èª¤
                    await db_service.update_paper_status(
                        db, file_id, "error", f"æª”æ¡ˆå¯¦é«”ä¸å­˜åœ¨ï¼Œå·²å˜—è©¦ {len(possible_paths)} å€‹è·¯å¾‘"
                    )
                    await self._cleanup_failed_task(file_id)
                    raise ValueError(f"æª”æ¡ˆå¯¦é«”ä¸å­˜åœ¨ï¼Œå·²å˜—è©¦ {len(possible_paths)} å€‹è·¯å¾‘")
                
                # æ§‹å»ºæª”æ¡ˆè³‡è¨Šå­—å…¸ï¼Œèˆ‡åŸ file_service æ ¼å¼ç›¸å®¹
                file_info = {
                    "file_id": file_id,
                    "file_path": file_path,
                    "filename": paper.file_name,
                    "original_filename": paper.original_filename,
                    "file_size": paper.file_size,
                    "status": "uploaded" if paper.processing_status in ["uploading", "uploaded"] else paper.processing_status,
                    "created_time": paper.created_at,
                    "modified_time": paper.upload_timestamp
                }
                
                logger.debug(f"æª”æ¡ˆé©—è­‰é€šé: {file_id}")
                return file_info
                
            except ValueError:
                # é‡æ–°æ‹‹å‡ºæˆ‘å€‘çš„æ¥­å‹™é‚è¼¯éŒ¯èª¤
                raise
            except Exception as e:
                logger.error(f"æª”æ¡ˆé©—è­‰æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {file_id} - {e}")
                await self._cleanup_failed_task(file_id)
                raise ValueError(f"æª”æ¡ˆé©—è­‰å¤±æ•—: {str(e)}")
    
    async def _cleanup_failed_task(self, file_id: str):
        """æ¸…ç†å¤±æ•—çš„ä»»å‹™ï¼Œé˜²æ­¢ç„¡é™é‡è©¦"""
        try:
            # 1. åœæ­¢ç›¸é—œçš„é‡è©¦ä»»å‹™
            logger.info(f"æ¸…ç†å¤±æ•—ä»»å‹™: {file_id}")
            
            # æ¸…ç†ä½‡åˆ—ä¸­çš„å¤±æ•—ä»»å‹™
            from .queue_service import queue_service
            await queue_service.cleanup_failed_tasks(file_id)
            
            # 2. æ›´æ–°è³‡æ–™åº«ç‹€æ…‹ï¼ˆå¦‚æœè¨˜éŒ„å­˜åœ¨ï¼‰
            from ..core.database import AsyncSessionLocal
            async with AsyncSessionLocal() as db:
                try:
                    await db_service.update_paper_status(
                        db, file_id, "error", "è™•ç†å¤±æ•—ï¼Œä»»å‹™å·²åœæ­¢"
                    )
                    logger.info(f"å·²æ›´æ–°è«–æ–‡ç‹€æ…‹ç‚ºéŒ¯èª¤: {file_id}")
                except Exception as e:
                    logger.warning(f"ç„¡æ³•æ›´æ–°è«–æ–‡ç‹€æ…‹: {file_id} - {e}")
            
            # 3. æ¸…ç†ç›¸é—œè³‡æº (å¦‚æœéœ€è¦çš„è©±)
            # ä¾‹å¦‚ï¼šæ¸…ç†æš«å­˜æª”æ¡ˆã€åœæ­¢ç›¸é—œæœå‹™ç­‰
            
        except Exception as e:
            logger.error(f"æ¸…ç†å¤±æ•—ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {file_id} - {e}")
    
    async def _process_with_grobid(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨ Grobid è™•ç†æª”æ¡ˆ"""
        file_path = file_info["file_path"]
        
        # æª¢æŸ¥ Grobid æœå‹™å¥åº·ç‹€æ…‹
        if not await grobid_service.health_check():
            raise Exception("Grobid æœå‹™ä¸å¯ç”¨")
        
        # å®Œæ•´è™•ç†
        complete_result = await grobid_service.process_paper_complete(file_path)

        if not complete_result.get("processing_success"):
            error_msg = complete_result.get('error_message', 'æœªçŸ¥çš„ Grobid è™•ç†éŒ¯èª¤')
            raise Exception(f"Grobid è™•ç†å¤±æ•—: {error_msg}")
        
        logger.info(f"Grobid è™•ç†å®Œæˆï¼Œæå–åˆ° {len(complete_result.get('sections', []))} å€‹ç« ç¯€")
        return complete_result
    
    async def _analyze_sections(
        self,
        grobid_result: Dict[str, Any],
        options: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ç« ç¯€åˆ†æ"""
        sections = grobid_result.get("sections", [])
        
        analyzed_sections = []
        
        for section in sections:
            # å¦‚æœåŸå§‹çš„section_idä¸æ˜¯UUIDæ ¼å¼ï¼Œç”Ÿæˆæ–°çš„UUID
            original_section_id = section.get("section_id", "")
            if original_section_id and len(original_section_id) < 32:  # ä¸æ˜¯UUIDæ ¼å¼
                section_id = str(uuid.uuid4())
            else:
                section_id = original_section_id or str(uuid.uuid4())
                
            section_analysis = {
                "section_id": section_id,
                "title": section.get("title", ""),
                "content": section.get("content", ""),
                "section_type": section.get("section_type", "other"),
                "page_num": section.get("page_num"),
                "word_count": section.get("word_count", 0),
                "order": section.get("order", 0),
                
                # åˆ†æçµæœ
                "sentences": [],
                "summary": "",
                "key_points": []
            }
            
            # å¥å­åˆ†å‰² (ä½¿ç”¨ Split Sentences æœå‹™)
            content = section.get("content", "")
            if content.strip():
                # ç‚ºé€™å€‹ç« ç¯€å‰µå»ºå–®å€‹ç« ç¯€åˆ—è¡¨
                single_section = [{
                    "section_type": section.get("section_type", "other"),
                    "content": content,
                    "page_start": section.get("page_num", 1)
                }]
                
                # èª¿ç”¨ split_sentences æœå‹™
                try:
                    processed_sentences = await split_sentences_service.process_sections_to_sentences(
                        single_section, 
                        language="mixed"
                    )
                    sentences = [s["text"] for s in processed_sentences]
                except Exception as e:
                    logger.warning(f"Split Sentences æœå‹™èª¿ç”¨å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•: {e}")
                    sentences = self._split_sentences(content)
            else:
                sentences = []
            
            section_analysis["sentences"] = sentences
            
            # ç”Ÿæˆæ‘˜è¦ (ç°¡åŒ–ç‰ˆ)
            if len(sentences) > 0:
                # å–å‰å…©å¥ä½œç‚ºæ‘˜è¦
                section_analysis["summary"] = " ".join(sentences[:2])
            
            analyzed_sections.append(section_analysis)
        
        logger.info(f"ç« ç¯€åˆ†æå®Œæˆï¼Œè™•ç† {len(analyzed_sections)} å€‹ç« ç¯€")
        return analyzed_sections
    
    async def _extract_sentences(self, sections_analysis: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ‰€æœ‰å¥å­"""
        sentences_data = []
        
        for section in sections_analysis:
            section_id = section["section_id"]
            section_title = section["title"]
            
            for sentence_text in section["sentences"]:
                if len(sentence_text.strip()) > 10:  # éæ¿¾å¤ªçŸ­çš„å¥å­
                    sentences_data.append({
                        "sentence_id": str(uuid.uuid4()),
                        "text": sentence_text.strip(),
                        "section_id": section_id,
                        "section_title": section_title,
                        "section_type": section["section_type"],
                        "word_count": len(sentence_text.split()),
                        "position_in_section": len([s for s in sentences_data if s.get("section_id") == section_id])
                    })
        
        logger.info(f"å¥å­æå–å®Œæˆï¼Œç¸½å…± {len(sentences_data)} å€‹å¥å­")
        return sentences_data
    
    async def _detect_od_cd(
        self,
        sentences_data: List[Dict[str, Any]],
        grobid_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """OD/CD æª¢æ¸¬ï¼Œå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶ï¼Œç¢ºä¿å–®å€‹å¤±æ•—ä¸å½±éŸ¿æ•´å€‹æ‰¹æ¬¡"""
        logger.info(f"é–‹å§‹OD/CDæª¢æ¸¬ï¼Œå¥å­ç¸½æ•¸: {len(sentences_data)}")
        
        # ä½¿ç”¨Semaphoreæ§åˆ¶ä¸¦ç™¼æ•¸
        semaphore = asyncio.Semaphore(n8n_service.max_concurrent_requests)
        
        async def detect_single_sentence_with_retry(idx: int, sentence_data: Dict[str, Any], max_retries: int = 3):
            """å–®å€‹å¥å­æª¢æ¸¬ï¼Œå¸¶æœ‰é‡è©¦æ©Ÿåˆ¶"""
            sentence_text = sentence_data["text"]
            
            for attempt in range(max_retries):
                async with semaphore:
                    try:
                        logger.debug(f"æª¢æ¸¬å¥å­ {idx}ï¼Œå˜—è©¦ {attempt + 1}/{max_retries}")
                        result = await n8n_service.detect_od_cd(
                            sentence=sentence_text,
                            cache_key=f"od_cd_{hash(sentence_text)}"
                        )
                        
                        # æˆåŠŸç²å¾—çµæœ
                        if "error" not in result and "defining_type" in result:
                            defining_type = result.get("defining_type", "UNKNOWN").upper()
                            is_od_cd = defining_type in ["OD", "CD"]
                            
                            # æ ¹æ“š defining_type è¨­ç½®å¸ƒæ—æ¬„ä½
                            has_objective = defining_type == "OD"
                            has_dataset = defining_type == "CD" and "dataset" in result.get("reason", "").lower()
                            has_contribution = defining_type == "CD" and "contribution" in result.get("reason", "").lower()
                            
                            # å¦‚æœæ˜¯ CD ä½†ç„¡æ³•ç´°åˆ†ï¼Œé è¨­ç‚º has_contribution
                            if defining_type == "CD" and not has_dataset and not has_contribution:
                                has_contribution = True
                            
                            return {
                                **sentence_data,
                                "is_od_cd": is_od_cd,
                                "confidence": 1.0 if is_od_cd else 0.0,
                                "od_cd_type": defining_type,
                                "explanation": result.get("reason", ""),
                                "detection_status": "success",
                                "retry_count": attempt,
                                # æ·»åŠ å¸ƒæ—æ¬„ä½ä¾›è³‡æ–™åº«å„²å­˜ä½¿ç”¨
                                "has_objective": has_objective,
                                "has_dataset": has_dataset,
                                "has_contribution": has_contribution
                            }
                        else:
                            # APIè¿”å›éŒ¯èª¤ï¼Œä½†ä¸é‡è©¦ï¼Œå› ç‚ºå¯èƒ½æ˜¯æ¥­å‹™é‚è¼¯å•é¡Œ
                            logger.warning(f"å¥å­ {idx} N8N APIè¿”å›éŒ¯èª¤: {result}")
                            break
                            
                    except Exception as e:
                        logger.warning(f"å¥å­ {idx} æª¢æ¸¬å¤±æ•—ï¼Œå˜—è©¦ {attempt + 1}/{max_retries}: {e}")
                        
                        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œç­‰å¾…å¾Œé‡è©¦
                        if attempt < max_retries - 1:
                            await asyncio.sleep(1 * (attempt + 1))  # éå¢ç­‰å¾…æ™‚é–“
                            continue
                        else:
                            # æœ€å¾Œä¸€æ¬¡å˜—è©¦ä¹Ÿå¤±æ•—äº†
                            logger.error(f"å¥å­ {idx} ç¶“é {max_retries} æ¬¡å˜—è©¦å¾Œä»ç„¶å¤±æ•—: {e}")
                            break
            
            # ä¸‰æ¬¡å˜—è©¦éƒ½å¤±æ•—ï¼Œæ¨™è¨˜ç‚ºéŒ¯èª¤
            return {
                **sentence_data,
                "is_od_cd": False,
                "confidence": 0.0,
                "od_cd_type": "ERROR",
                "explanation": f"APIèª¿ç”¨å¤±æ•—ï¼Œç¶“é {max_retries} æ¬¡é‡è©¦",
                "detection_status": "error",
                "retry_count": max_retries,
                # éŒ¯èª¤æƒ…æ³ä¸‹çš„å¸ƒæ—æ¬„ä½
                "has_objective": False,
                "has_dataset": False,
                "has_contribution": False
            }
        
        # ç‚ºæ¯å€‹å¥å­å‰µå»ºæª¢æ¸¬ä»»å‹™
        detection_tasks = []
        for idx, sentence_data in enumerate(sentences_data):
            task = detect_single_sentence_with_retry(idx, sentence_data)
            detection_tasks.append(task)
        
        logger.info(f"ä¸¦ç™¼åŸ·è¡Œ {len(detection_tasks)} å€‹OD/CDæª¢æ¸¬ä»»å‹™ï¼Œæœ€å¤§ä¸¦ç™¼æ•¸: {n8n_service.max_concurrent_requests}")
        
        # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰æª¢æ¸¬ä»»å‹™ï¼Œä½¿ç”¨return_exceptions=Trueç¢ºä¿å–®å€‹å¤±æ•—ä¸å½±éŸ¿å…¶ä»–
        detection_results = await asyncio.gather(*detection_tasks, return_exceptions=True)
        
        # è™•ç†çµæœ
        all_results = []
        successful_count = 0
        error_count = 0
        
        for idx, result in enumerate(detection_results):
            if isinstance(result, Exception):
                # å¦‚æœä»»å‹™æœ¬èº«æ‹‹å‡ºç•°å¸¸ï¼ˆä¸å¤ªå¯èƒ½ï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“è™•ç†äº†æ‰€æœ‰ç•°å¸¸ï¼‰
                logger.error(f"å¥å­ {idx} ä»»å‹™ç•°å¸¸: {result}")
                sentence_data = sentences_data[idx]
                combined_result = {
                    **sentence_data,
                    "is_od_cd": False,
                    "confidence": 0.0,
                    "od_cd_type": "ERROR",
                    "explanation": f"ä»»å‹™åŸ·è¡Œç•°å¸¸: {str(result)}",
                    "detection_status": "error",
                    "retry_count": 3,
                    # ç•°å¸¸æƒ…æ³ä¸‹çš„å¸ƒæ—æ¬„ä½
                    "has_objective": False,
                    "has_dataset": False,
                    "has_contribution": False
                }
                error_count += 1
            else:
                combined_result = result
                if result.get("detection_status") == "success":
                    successful_count += 1
                elif result.get("detection_status") == "error":
                    error_count += 1
            
            all_results.append(combined_result)
        
        detected_count = sum(1 for r in all_results if r.get("is_od_cd"))
        
        logger.info(f"OD/CD æª¢æ¸¬å®Œæˆ:")
        logger.info(f"  - ç¸½å¥å­æ•¸: {len(all_results)}")
        logger.info(f"  - æˆåŠŸæª¢æ¸¬: {successful_count}")
        logger.info(f"  - æª¢æ¸¬å¤±æ•—: {error_count}")
        logger.info(f"  - OD/CDå¥å­: {detected_count}")
        
        return all_results
    
    async def _extract_keywords(self, sections_analysis: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é—œéµè©æå–"""
        all_keywords = []
        
        for section in sections_analysis:
            content = section["content"]
            
            if len(content.strip()) > 50:  # åªè™•ç†æœ‰è¶³å¤ å…§å®¹çš„ç« ç¯€
                try:
                    result = await n8n_service.extract_keywords(
                        query=content,
                    )
                    
                    if "error" not in result and "keywords" in result:
                        section_keywords = {
                            "section_id": section["section_id"],
                            "section_title": section["title"],
                            "section_type": section["section_type"],
                            "keywords": result["keywords"],
                            "keyword_count": len(result["keywords"]),
                            "extraction_confidence": result.get("confidence", 0.0)
                        }
                        all_keywords.append(section_keywords)
                    else:
                        logger.warning(f"é—œéµè©æå–å¤±æ•—: {section['section_id']} - {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                
                except Exception as e:
                    logger.error(f"é—œéµè©æå–ç•°å¸¸: {section['section_id']} - {e}")
                
                # é¿å… API é™æµ
                # await asyncio.sleep(0.3)  # âœ… æš«æ™‚è¨»è§£æ‰å»¶é²
        
        total_keywords = sum(len(k["keywords"]) for k in all_keywords)
        logger.info(f"é—œéµè©æå–å®Œæˆï¼Œå¾ {len(all_keywords)} å€‹ç« ç¯€æå– {total_keywords} å€‹é—œéµè©")
        
        return all_keywords
    
    async def _store_results(
        self,
        paper_id: str,
        grobid_result: Dict[str, Any],
        sections_analysis: List[Dict[str, Any]],
        sentences_data: List[Dict[str, Any]],
        od_cd_results: Optional[List[Dict[str, Any]]] = None,
        keyword_results: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨SQLAlchemy ORMå°‡è™•ç†çµæœå­˜å„²åˆ°è³‡æ–™åº«"""
        async with AsyncSessionLocal() as session:
            try:
                # 1. æ›´æ–°Paperè¨˜éŒ„
                paper_update_data = {
                    "processing_status": "completed",
                    "grobid_processed": True,
                    "sentences_processed": True,
                    "tei_xml": grobid_result.get("tei_xml"),
                    "tei_metadata": {
                        "title": grobid_result.get("title"),
                        "authors": grobid_result.get("authors", []),
                        "abstract": grobid_result.get("abstract"),
                    },
                    "processing_completed_at": datetime.now()
                }
                
                await session.execute(
                    update(Paper)
                    .where(Paper.id == paper_id)
                    .values(**paper_update_data)
                )

                # 2. æº–å‚™ä¸¦æ‰¹æ¬¡æ’å…¥ç« ç¯€ (PaperSection)
                sections_to_insert = []
                for section_data in sections_analysis:
                    sections_to_insert.append({
                        "id": section_data["section_id"],
                        "paper_id": paper_id,
                        "section_type": section_data["section_type"],
                        "page_num": section_data.get("page_num"),
                        "content": section_data["content"],
                        "section_order": section_data.get("order"),
                        "word_count": section_data.get("word_count"),
                    })

                if sections_to_insert:
                    stmt = pg_insert(PaperSection).values(sections_to_insert)
                    stmt = stmt.on_conflict_do_update(
                        index_elements=['id'],
                        set_={
                            "section_type": stmt.excluded.section_type,
                            "page_num": stmt.excluded.page_num,
                            "content": stmt.excluded.content,
                            "section_order": stmt.excluded.section_order,
                            "word_count": stmt.excluded.word_count,
                        }
                    )
                    await session.execute(stmt)

                # 3. æº–å‚™ä¸¦æ‰¹æ¬¡æ’å…¥å¥å­ (Sentence)
                sentences_to_insert = []
                if od_cd_results:
                    for sentence_data in od_cd_results:
                        sentences_to_insert.append({
                            "id": sentence_data["sentence_id"],
                            "paper_id": paper_id,
                            "section_id": sentence_data["section_id"],
                            "content": sentence_data.get("sentence_text", sentence_data.get("text", "")),
                            "sentence_order": sentence_data.get("order"),
                            "word_count": len(sentence_data.get("sentence_text", sentence_data.get("text", "")).split()),
                            "detection_status": sentence_data.get("detection_status", "completed"),
                            "error_message": sentence_data.get("error_message"),
                            "retry_count": sentence_data.get("retry_count", 0),
                            "explanation": sentence_data.get("explanation"),
                            "has_objective": sentence_data.get("has_objective"),
                            "has_dataset": sentence_data.get("has_dataset"),
                            "has_contribution": sentence_data.get("has_contribution"),
                        })
                
                if sentences_to_insert:
                    stmt = pg_insert(Sentence).values(sentences_to_insert)
                    stmt = stmt.on_conflict_do_update(
                        index_elements=['id'],
                        set_={
                            "content": stmt.excluded.content,
                            "sentence_order": stmt.excluded.sentence_order,
                            "word_count": stmt.excluded.word_count,
                            "detection_status": stmt.excluded.detection_status,
                            "error_message": stmt.excluded.error_message,
                            "retry_count": stmt.excluded.retry_count,
                            "explanation": stmt.excluded.explanation,
                            "has_objective": stmt.excluded.has_objective,
                            "has_dataset": stmt.excluded.has_dataset,
                            "has_contribution": stmt.excluded.has_contribution,
                        }
                    )
                    await session.execute(stmt)

                await session.commit()
                
                storage_result = {
                    "stored_at": datetime.now().isoformat(),
                    "sections_stored": len(sections_to_insert),
                    "sentences_stored": len(sentences_to_insert),
                    "keywords_stored": 0 # é—œéµè©å­˜å„²é‚è¼¯å¾…å¯¦ç¾
                }
                
                logger.info(f"è™•ç†çµæœå·²å­˜å„²: {paper_id}")
                return storage_result

            except Exception as e:
                await session.rollback()
                logger.error(f"å­˜å„²è™•ç†çµæœå¤±æ•—: {paper_id} - {e}", exc_info=True)
                # æ›´æ–°Paperç‹€æ…‹ç‚ºéŒ¯èª¤
                await session.execute(
                    update(Paper)
                    .where(Paper.id == paper_id)
                    .values(processing_status='error', error_message=f"Store results failed: {e}")
                )
                await session.commit()
                raise
    
    async def _cleanup_temp_files(self, file_info: Dict[str, Any]):
        """æ¸…ç†æš«å­˜æª”æ¡ˆ"""
        try:
            file_path = file_info.get("file_path")
            if file_path and os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"æš«å­˜æª”æ¡ˆå·²æ¸…ç†: {file_path}")
        except Exception as e:
            logger.warning(f"æ¸…ç†æš«å­˜æª”æ¡ˆå¤±æ•—: {e}")
    
    async def _verify_processing_completion(self, db: AsyncSession, paper_id: str) -> Dict[str, Any]:
        """é©—è­‰è«–æ–‡è™•ç†æ˜¯å¦çœŸæ­£å®Œæˆ - é—œéµçš„è³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥ï¼ˆå¢å¼·ç‰ˆï¼‰"""
        try:
            # ğŸ”„ å¢åŠ é‡è©¦æ©Ÿåˆ¶ä¾†è™•ç†äº‹å‹™éš”é›¢å•é¡Œ
            max_retries = 3
            retry_delay = 1.0
            
            for attempt in range(max_retries):
                try:
                    # ğŸ›¡ï¸ å¼·åˆ¶åˆ·æ–°æœƒè©±ï¼Œç¢ºä¿è®€å–æœ€æ–°æ•¸æ“š
                    if hasattr(db, 'expire_all'):
                        db.expire_all()
                    
                    # 1. æª¢æŸ¥è«–æ–‡è¨˜éŒ„ï¼ˆä½¿ç”¨ fresh æŸ¥è©¢ï¼‰
                    from sqlalchemy import text
                    fresh_paper_query = text("""
                        SELECT id, tei_xml, grobid_processed, sentences_processed, od_cd_processed, processing_status
                        FROM papers 
                        WHERE id = :paper_id
                    """)
                    result = await db.execute(fresh_paper_query, {"paper_id": paper_id})
                    paper_row = result.fetchone()
                    
                    if not paper_row:
                        return {"success": False, "error": "è«–æ–‡è¨˜éŒ„ä¸å­˜åœ¨"}
                    
                    paper_id_db, tei_xml, grobid_processed, sentences_processed, od_cd_processed, processing_status = paper_row
                    
                    # ğŸ“Š è©³ç´°è¨ºæ–·æ—¥èªŒ
                    logger.info(f"[é©—è­‰] å˜—è©¦ {attempt + 1}/{max_retries} - è«–æ–‡ {paper_id}")
                    logger.info(f"[é©—è­‰] TEI XML é•·åº¦: {len(tei_xml) if tei_xml else 0}")
                    logger.info(f"[é©—è­‰] è™•ç†ç‹€æ…‹: grobid={grobid_processed}, sentences={sentences_processed}, od_cd={od_cd_processed}")
                    logger.info(f"[é©—è­‰] è™•ç†ç‹€æ…‹æ¨™è¨˜: {processing_status}")
                    
                    # 2. æª¢æŸ¥ TEI XML
                    if not tei_xml or len(tei_xml) < 1000:
                        if attempt < max_retries - 1:
                            logger.warning(f"[é©—è­‰] TEI XML æª¢æŸ¥å¤±æ•— (å˜—è©¦ {attempt + 1}): é•·åº¦={len(tei_xml) if tei_xml else 0}, é‡è©¦ä¸­...")
                            await asyncio.sleep(retry_delay)
                            continue
                        else:
                            logger.error(f"[é©—è­‰] TEI XML æœ€çµ‚æª¢æŸ¥å¤±æ•—: é•·åº¦={len(tei_xml) if tei_xml else 0}")
                            return {"success": False, "error": "TEI XML ç¼ºå¤±æˆ–å¤ªçŸ­"}
                    
                    # 3. æª¢æŸ¥ç« ç¯€è³‡æ–™ï¼ˆä½¿ç”¨ fresh æŸ¥è©¢ï¼‰
                    sections_query = text("SELECT COUNT(*) FROM paper_sections WHERE paper_id = :paper_id")
                    sections_result = await db.execute(sections_query, {"paper_id": paper_id})
                    actual_sections = sections_result.scalar()
                    
                    logger.info(f"[é©—è­‰] ç« ç¯€æ•¸é‡: {actual_sections}")
                    
                    if actual_sections == 0:
                        if attempt < max_retries - 1:
                            logger.warning(f"[é©—è­‰] ç« ç¯€æª¢æŸ¥å¤±æ•— (å˜—è©¦ {attempt + 1}): æ•¸é‡=0, é‡è©¦ä¸­...")
                            await asyncio.sleep(retry_delay)
                            continue
                        else:
                            return {"success": False, "error": "ç« ç¯€è³‡æ–™å®Œå…¨ç¼ºå¤±"}
                    
                    # 4. æª¢æŸ¥å¥å­è³‡æ–™ï¼ˆä½¿ç”¨ fresh æŸ¥è©¢ï¼‰
                    sentences_query = text("SELECT COUNT(*) FROM sentences WHERE paper_id = :paper_id")
                    sentences_result = await db.execute(sentences_query, {"paper_id": paper_id})
                    actual_sentences = sentences_result.scalar()
                    
                    logger.info(f"[é©—è­‰] å¥å­æ•¸é‡: {actual_sentences}")
                    
                    if actual_sentences == 0:
                        if attempt < max_retries - 1:
                            logger.warning(f"[é©—è­‰] å¥å­æª¢æŸ¥å¤±æ•— (å˜—è©¦ {attempt + 1}): æ•¸é‡=0, é‡è©¦ä¸­...")
                            await asyncio.sleep(retry_delay)
                            continue
                        else:
                            return {"success": False, "error": "å¥å­è³‡æ–™å®Œå…¨ç¼ºå¤±"}
                    
                    # 5. æª¢æŸ¥ç« ç¯€å’Œå¥å­çš„é—œè¯æ€§
                    orphan_query = text("""
                        SELECT COUNT(*) FROM sentences s 
                        WHERE s.paper_id = :paper_id 
                        AND s.section_id NOT IN (
                            SELECT ps.id FROM paper_sections ps WHERE ps.paper_id = :paper_id
                        )
                    """)
                    orphan_result = await db.execute(orphan_query, {"paper_id": paper_id})
                    orphan_sentences = orphan_result.scalar()
                    
                    if orphan_sentences > 0:
                        logger.warning(f"[é©—è­‰] ç™¼ç¾ {orphan_sentences} å€‹å­¤ç«‹å¥å­")
                        if orphan_sentences > actual_sentences * 0.5:  # è¶…é50%æ‰ç®—éŒ¯èª¤
                            return {"success": False, "error": f"éå¤šå­¤ç«‹å¥å­: {orphan_sentences}/{actual_sentences}"}
                    
                    # 6. æª¢æŸ¥è™•ç†ç‹€æ…‹æ¨™è¨˜
                    if not grobid_processed:
                        logger.error(f"[é©—è­‰] Grobid è™•ç†ç‹€æ…‹æœªæ¨™è¨˜ç‚ºå®Œæˆ: {grobid_processed}")
                        return {"success": False, "error": "Grobid è™•ç†ç‹€æ…‹æœªæ¨™è¨˜ç‚ºå®Œæˆ"}
                    
                    if not sentences_processed:
                        logger.error(f"[é©—è­‰] å¥å­è™•ç†ç‹€æ…‹æœªæ¨™è¨˜ç‚ºå®Œæˆ: {sentences_processed}")
                        return {"success": False, "error": "å¥å­è™•ç†ç‹€æ…‹æœªæ¨™è¨˜ç‚ºå®Œæˆ"}
                    
                    # 7. æª¢æŸ¥å…§å®¹è³ªé‡
                    empty_sections_query = text("""
                        SELECT COUNT(*) FROM paper_sections 
                        WHERE paper_id = :paper_id AND (content IS NULL OR LENGTH(content) < 10)
                    """)
                    empty_sections_result = await db.execute(empty_sections_query, {"paper_id": paper_id})
                    empty_sections = empty_sections_result.scalar()
                    
                    if empty_sections > actual_sections * 0.5:
                        logger.warning(f"[é©—è­‰] éå¤šç©ºç™½ç« ç¯€: {empty_sections}/{actual_sections}")
                        return {"success": False, "error": f"éå¤šç©ºç™½ç« ç¯€: {empty_sections}/{actual_sections}"}
                    
                    # 8. æª¢æŸ¥å¥å­å…§å®¹è³ªé‡
                    empty_sentences_query = text("""
                        SELECT COUNT(*) FROM sentences 
                        WHERE paper_id = :paper_id AND (content IS NULL OR LENGTH(content) < 5)
                    """)
                    empty_sentences_result = await db.execute(empty_sentences_query, {"paper_id": paper_id})
                    empty_sentences = empty_sentences_result.scalar()
                    
                    if empty_sentences > actual_sentences * 0.3:
                        logger.warning(f"[é©—è­‰] éå¤šç©ºç™½å¥å­: {empty_sentences}/{actual_sentences}")
                        return {"success": False, "error": f"éå¤šç©ºç™½å¥å­: {empty_sentences}/{actual_sentences}"}
                    
                    # ğŸ‰ æ‰€æœ‰æª¢æŸ¥é€šé
                    summary = {
                        "paper_id": paper_id,
                        "tei_xml_length": len(tei_xml),
                        "sections_count": actual_sections,
                        "sentences_count": actual_sentences,
                        "empty_sections": empty_sections,
                        "empty_sentences": empty_sentences,
                        "orphan_sentences": orphan_sentences,
                        "grobid_processed": grobid_processed,
                        "sentences_processed": sentences_processed,
                        "od_cd_processed": od_cd_processed,
                        "verification_attempts": attempt + 1
                    }
                    
                    logger.info(f"[é©—è­‰] âœ… é©—è­‰æˆåŠŸ (å˜—è©¦ {attempt + 1}): {actual_sections} ç« ç¯€, {actual_sentences} å¥å­")
                    
                    return {
                        "success": True, 
                        "summary": f"é©—è­‰é€šé: {actual_sections} ç« ç¯€, {actual_sentences} å¥å­ (é‡è©¦ {attempt + 1} æ¬¡)",
                        "details": summary
                    }
                    
                except Exception as retry_error:
                    if attempt < max_retries - 1:
                        logger.warning(f"[é©—è­‰] å˜—è©¦ {attempt + 1} ç™¼ç”ŸéŒ¯èª¤ï¼Œé‡è©¦ä¸­: {retry_error}")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•¸é€€é¿
                        continue
                    else:
                        raise retry_error
            
            # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
            return {"success": False, "error": f"é©—è­‰å¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡"}
            
        except Exception as e:
            logger.error(f"[é©—è­‰] è™•ç†å®Œæˆé©—è­‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return {"success": False, "error": f"é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}
    
    # ===== æ‰¹æ¬¡è™•ç†åŠŸèƒ½ =====
    
    async def batch_process_files(
        self,
        file_ids: List[str],
        user_id: str = None,
        priority: TaskPriority = TaskPriority.NORMAL,
        options: Dict[str, Any] = None
    ) -> List[str]:
        """æ‰¹æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ"""
        task_ids = []
        
        for file_id in file_ids:
            task_id = await self.process_file(
                file_id=file_id,
                user_id=user_id,
                priority=priority,
                options=options
            )
            task_ids.append(task_id)
        
        logger.info(f"æ‰¹æ¬¡è™•ç†ä»»å‹™å·²å‰µå»º: {len(task_ids)} å€‹æª”æ¡ˆ")
        return task_ids
    
    async def _batch_sentence_analysis(self, task: QueueTask) -> Dict[str, Any]:
        """æ‰¹æ¬¡å¥å­åˆ†æä»»å‹™"""
        sentences = task.data["sentences"]
        analysis_type = task.data.get("analysis_type", "od_cd_detection")
        
        logger.info(f"é–‹å§‹æ‰¹æ¬¡å¥å­åˆ†æ: {len(sentences)} å€‹å¥å­")
        
        if analysis_type == "od_cd_detection":
            # ä¿®æ­£ï¼šé€å¥è™•ç†è€Œéæ‰¹æ¬¡å‚³é
            sentence_results = []
            for sentence in sentences:
                try:
                    result = await n8n_service.detect_od_cd(
                        sentence=sentence,  # âœ… æ­£ç¢ºçš„å–®å¥èª¿ç”¨
                        cache_key=f"batch_analysis_{hash(sentence)}"
                    )
                    sentence_results.append({
                        "sentence": sentence,
                        "result": result
                    })
                except Exception as e:
                    logger.warning(f"æ‰¹æ¬¡åˆ†æå–®å¥å¤±æ•—: {e}")
                    sentence_results.append({
                        "sentence": sentence,
                        "result": {"error": str(e)}
                    })
                
                # æ§åˆ¶APIèª¿ç”¨é »ç‡
                # await asyncio.sleep(0.1)  # âœ… æš«æ™‚è¨»è§£æ‰å»¶é²
            
            return {
                "results": sentence_results,
                "total_sentences": len(sentences),
                "successful_detections": len([r for r in sentence_results if "error" not in r["result"]])
            }
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†æé¡å‹: {analysis_type}")
        
        return result
    
    async def _reprocess_sections(self, task: QueueTask) -> Dict[str, Any]:
        """é‡æ–°è™•ç†ç‰¹å®šç« ç¯€"""
        file_id = task.data["file_id"]
        section_ids = task.data["section_ids"]
        
        logger.info(f"é‡æ–°è™•ç†ç« ç¯€: {file_id} - {section_ids}")
        
        # å¯¦ä½œç« ç¯€é‡æ–°è™•ç†é‚è¼¯
        # é€™è£¡å¯ä»¥é‡æ–°æå–é—œéµè©,é‡æ–°åˆ†æå…§å®¹ç­‰
        
        return {"reprocessed_sections": section_ids}
    
    # ===== å·¥å…·æ–¹æ³• =====
    
    def _split_sentences(self, text: str) -> List[str]:
        """ç°¡å–®çš„å¥å­åˆ†å‰²"""
        import re
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²å¥å­
        sentences = re.split(r'[.!?]+', text)
        
        # æ¸…ç†å’Œéæ¿¾
        clean_sentences = []
        for sentence in sentences:
            clean_sentence = sentence.strip()
            if len(clean_sentence) > 10:  # éæ¿¾å¤ªçŸ­çš„å¥å­
                clean_sentences.append(clean_sentence)
        
        return clean_sentences
    
    # ===== ç‹€æ…‹æŸ¥è©¢ =====
    
    async def get_processing_status(self, file_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–æª”æ¡ˆè™•ç†ç‹€æ…‹"""
        # æŸ¥æ‰¾èˆ‡æª”æ¡ˆç›¸é—œçš„ä»»å‹™
        all_tasks = []
        
        # æª¢æŸ¥ä½‡åˆ—ä¸­çš„ä»»å‹™
        queue_status = await queue_service.get_queue_status()
        
        # å¾è³‡æ–™åº«æŸ¥è©¢ä»»å‹™
        try:
            cursor = await db_service.get_connection()
            result = await cursor.execute(
                """SELECT task_data FROM queue_tasks 
                   WHERE JSON_EXTRACT(task_data, '$.file_id') = ?
                   ORDER BY JSON_EXTRACT(task_data, '$.created_at') DESC
                   LIMIT 1""",
                (file_id,)
            )
            
            row = await result.fetchone()
            if row:
                task_data = json.loads(row[0])
                task = queue_service.QueueTask.from_dict(task_data)
                
                return {
                    "file_id": file_id,
                    "task_id": task.task_id,
                    "status": task.status.value,
                    "progress": task.progress.to_dict() if hasattr(task.progress, 'to_dict') else asdict(task.progress),
                    "created_at": task.created_at.isoformat(),
                    "started_at": task.started_at.isoformat() if task.started_at else None,
                    "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                    "error_message": task.error_message,
                    "result_summary": task.result
                }
        
        except Exception as e:
            logger.error(f"æŸ¥è©¢è™•ç†ç‹€æ…‹å¤±æ•—: {e}")
        
        return None

    async def _update_section_page_numbers(self, session: AsyncSession, paper_id: str, pdf_path: str) -> bool:
        """
        æ›´æ–°ç¾æœ‰ç« ç¯€çš„é ç¢¼è³‡è¨Š
        
        Args:
            session: è³‡æ–™åº«æœƒè©±
            paper_id: è«–æ–‡ID
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘
            
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            logger.info(f"é–‹å§‹æ›´æ–°ç« ç¯€é ç¢¼è³‡è¨Š: {paper_id}")
            
            # æª¢æŸ¥ PDF æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not Path(pdf_path).exists():
                logger.warning(f"PDF æª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³éé ç¢¼æ›´æ–°: {pdf_path}")
                return False
            
            # ç²å–ç¾æœ‰ç« ç¯€
            existing_sections = await db_service.get_sections_for_paper(session, paper_id)
            if not existing_sections:
                logger.warning(f"æ²’æœ‰æ‰¾åˆ°ç¾æœ‰ç« ç¯€: {paper_id}")
                return False
            
            logger.info(f"æ‰¾åˆ° {len(existing_sections)} å€‹ç¾æœ‰ç« ç¯€ï¼Œé–‹å§‹ PDF åˆ†æ")
            
            # é€²è¡Œ PDF åˆ†æ
            from .pdf_analyzer import pdf_analyzer
            pdf_analysis = await pdf_analyzer.analyze_pdf(pdf_path)
            
            if not pdf_analysis or not pdf_analysis.get('text_blocks'):
                logger.warning(f"PDF åˆ†æå¤±æ•—æˆ–æ²’æœ‰æ–‡æœ¬å¡Š: {paper_id}")
                return False
            
            logger.info(f"PDF åˆ†æå®Œæˆï¼Œå…± {pdf_analysis['total_pages']} é ï¼Œ{len(pdf_analysis['text_blocks'])} å€‹æ–‡æœ¬å¡Š")
            
            # æ›´æ–°æ¯å€‹ç« ç¯€çš„é ç¢¼
            updated_count = 0
            for i, section in enumerate(existing_sections):
                try:
                    # ä½¿ç”¨ PDF åˆ†æä¾†ç¢ºå®šé ç¢¼
                    page_num = await self._determine_page_number_for_section(
                        section, i, len(existing_sections), pdf_analysis
                    )
                    
                    if page_num and page_num != section.page_num:
                        # æ›´æ–°è³‡æ–™åº«ä¸­çš„é ç¢¼
                        await db_service.update_section_page_number(session, section.id, page_num)
                        updated_count += 1
                        logger.info(f"æ›´æ–°ç« ç¯€ {i+1} é ç¢¼: {section.page_num} -> {page_num}")
                    
                except Exception as e:
                    logger.error(f"æ›´æ–°ç« ç¯€ {i+1} é ç¢¼å¤±æ•—: {e}")
                    continue
            
            await session.commit()
            logger.info(f"é ç¢¼æ›´æ–°å®Œæˆ: {paper_id}, æ›´æ–°äº† {updated_count} å€‹ç« ç¯€")
            return updated_count > 0
            
        except Exception as e:
            logger.error(f"é ç¢¼æ›´æ–°éç¨‹å¤±æ•—: {paper_id} - {e}", exc_info=True)
            await session.rollback()
            return False
    
    async def _determine_page_number_for_section(
        self, 
        section: Any, 
        section_index: int, 
        total_sections: int, 
        pdf_analysis: Dict[str, Any]
    ) -> Optional[int]:
        """
        ç‚ºå–®å€‹ç« ç¯€ç¢ºå®šé ç¢¼
        
        Args:
            section: ç« ç¯€å°è±¡
            section_index: ç« ç¯€ç´¢å¼•
            total_sections: ç¸½ç« ç¯€æ•¸
            pdf_analysis: PDF åˆ†æçµæœ
            
        Returns:
            é ç¢¼
        """
        try:
            from .pdf_analyzer import pdf_analyzer
            
            # æ–¹æ³•1: é€šéå…§å®¹åŒ¹é…
            if hasattr(section, 'content') and section.content:
                content_sample = section.content[:100] if len(section.content) > 100 else section.content
                content_page = pdf_analyzer.find_text_page(content_sample, pdf_analysis['text_blocks'])
                if content_page:
                    logger.debug(f"ç« ç¯€ {section_index+1} é€šéå…§å®¹åŒ¹é…ç²å¾—é ç¢¼: {content_page}")
                    return content_page
            
            # æ–¹æ³•2: åŸºæ–¼ PDF ç¸½é æ•¸çš„æ™ºèƒ½ä¼°ç®—
            if pdf_analysis.get('total_pages'):
                estimated_page = pdf_analyzer.estimate_page_from_position(
                    section_index, total_sections, pdf_analysis['total_pages']
                )
                logger.debug(f"ç« ç¯€ {section_index+1} åŸºæ–¼ä½ç½®ä¼°ç®—é ç¢¼: {estimated_page}")
                return estimated_page
            
            # æ–¹æ³•3: ç°¡å–®ä¼°ç®—ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
            estimated_page = max(1, (section_index // 2) + 1)
            logger.debug(f"ç« ç¯€ {section_index+1} ä½¿ç”¨ç°¡å–®ä¼°ç®—é ç¢¼: {estimated_page}")
            return estimated_page
            
        except Exception as e:
            logger.error(f"ç¢ºå®šç« ç¯€é ç¢¼å¤±æ•—: {e}")
            return max(1, (section_index // 2) + 1)  # å‚™ç”¨æ–¹æ¡ˆ

    async def _get_tei_xml_with_retry(self, session: AsyncSession, paper_id: str, max_retries: int = 5, delay: float = 1.0) -> str:
        """å˜—è©¦å¾è³‡æ–™åº«å–å¾— TEI XMLï¼Œé¿å…å› äº¤æ˜“æœªæäº¤é€ æˆè®€å–å¤±æ•—"""
        for attempt in range(max_retries):
            tei_xml = await db_service.get_paper_tei_xml(session, paper_id)
            if tei_xml:
                return tei_xml
            if attempt < max_retries - 1:
                await asyncio.sleep(delay)
            else:
                raise ValueError(f"ç„¡æ³•å¾è³‡æ–™åº«ç²å– TEI XML (é‡è©¦ {max_retries} æ¬¡å¾Œå¤±æ•—): {paper_id}")

# å»ºç«‹æœå‹™å¯¦ä¾‹
processing_service = ProcessingService()
